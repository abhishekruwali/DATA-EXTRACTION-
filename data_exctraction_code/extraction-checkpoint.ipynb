{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b880d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Input, Embedding, Bidirectional, LSTM, Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af00d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "### kewyord based extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['cancer','Tumor','Neoplasm','Carcinoma','Metastasis','Oncology','Chemotherapy',\n",
    "'Radiation','Immunotherapy', 'mutation','Prognosis','Stage','Palliative','Mammogram','Mastectomy','Lumpectomy',\n",
    "'HER2/neu','Triple-negative''breast','carcinoma','Smoker','Bronchoscopy','Thoracoscopy','Adenocarcinoma',\n",
    "'Squamous','Mesothelioma','Gleason',\n",
    "'prostatectomy','Brachytherapy','Androgen', 'Prostatectomy','Melanoma','Basal',\n",
    "'Squamous','UV','Sunscreen','Mohs','Sentinel','Whipple','Pancreaticoduodenectomy',  \n",
    "'cholangiopancreatography', 'neuroendocrine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d55c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('impure_test_unstructured.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f947fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentences related to cancer\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "cancer_sentences = []\n",
    "for sentence in sentences:\n",
    "    if any(keyword in sentence.lower() for keyword in keywords):\n",
    "        cancer_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3411420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the sentences in a new txt file\n",
    "with open('keyword_output1.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in cancer_sentences:\n",
    "            f.write(sentence + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fb375",
   "metadata": {},
   "outputs": [],
   "source": [
    "### regular expression based extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('impure_test_unstructured.txt', 'r',encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43790d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regular expression to match cancer-related terms\n",
    "cancer_regex = re.compile(r'cancer|malignancy|tumor|''Radiation'|'mutation', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_paragraphs = [p for p in text.split('\\n\\n') if re.search(cancer_regex, p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e0426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the sentences in a new txt file\n",
    "with open('rule_output.txt', 'w',encoding='utf-8') as f:\n",
    "   for p in cancer_paragraphs:\n",
    "        f.write(p + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b843265",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deep learning based extarction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('input.xlsx')\n",
    "data = data.dropna(subset=['text'])\n",
    "data['text'] = data['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "test_data['label'] = pd.to_numeric(test_data['label'], errors='coerce').fillna(-1).astype(int)\n",
    "train_data['label'] = train_data['label'].astype(int)\n",
    "test_data['label'] = test_data['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "num_classes = 1\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text data to sequences of integers and pad them\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['text'].values)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['text'].values)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573935b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deep learning model\n",
    "inputs = Input(shape=(max_length,))\n",
    "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length)(inputs)\n",
    "bi_lstm_layer = Bidirectional(LSTM(64, return_sequences=False))(embedding_layer)\n",
    "output_layer = Dense(num_classes, activation='sigmoid')(bi_lstm_layer)\n",
    "model = Model(inputs=inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dad341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e327ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_padded, train_data['label'].values, epochs=4, batch_size=32, validation_data=(test_padded, test_data['label'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79714c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('impure_test_unstructured.txt', 'r',encoding='utf-8') as f:\n",
    "    new_data = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47af304",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sequences = tokenizer.texts_to_sequences(new_data)\n",
    "new_padded = pad_sequences(new_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(new_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b799b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dl_output1.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.5:\n",
    "            f.write(new_data[i] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
